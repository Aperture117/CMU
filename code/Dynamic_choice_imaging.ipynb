{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"R59_xprhad6I","executionInfo":{"status":"error","timestamp":1745350534858,"user_tz":240,"elapsed":2897,"user":{"displayName":"Hyunseok Lee","userId":"04566572328476018748"}},"outputId":"c415dfc6-45e7-4e82-c9dc-778ddf56f1b1","colab":{"base_uri":"https://localhost:8080/","height":383}},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'process_behavior_and_movement_data'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e46a0bdcb879>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprocess_behavior_and_movement_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSessionDataProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0manalyze_behavior_data_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBehaviorDataAnalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMovementAnalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBehaviorPlotter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGLMAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'process_behavior_and_movement_data'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pathlib import Path\n","import scipy.io as sio\n","from process_behavior_and_movement_data import SessionDataProcessor\n","from analyze_behavior_data_functions import BehaviorDataAnalyzer, MovementAnalyzer, BehaviorPlotter, GLMAnalyzer\n","import matplotlib as mpl\n","import scipy\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","#Remove right and top spines from plots (personal preference)\n","mpl.rcParams['axes.spines.right'] = False\n","mpl.rcParams['axes.spines.top'] = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-caQ0vnad6K"},"outputs":[],"source":["# Initialize the analyzers\n","behavior_analyzer = BehaviorDataAnalyzer(base_dir='/Volumes/Runyan5/Akhil/behavior/')\n","movement_analyzer = MovementAnalyzer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ElMgt5Q2ad6K"},"outputs":[],"source":["figures_dir = \"figures\"\n","data_dir = 'dynamic_choice'"]},{"cell_type":"markdown","metadata":{"id":"fLy3GZJkad6K"},"source":["# Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_fUG0fOAad6L"},"outputs":[],"source":["mouse_list = ['IS-2-1L']\n","\n","date_lists = {\n","    'IS-2-1L': ['250314', '250320', '250401', '250402', '250404', '250408'],\n","}\n","\n","#date_lists = {\n","#    'IS-2-1L': ['250401', '250402', '250404', '250408'],\n","#}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXvW4pnyad6L"},"outputs":[],"source":["# Initialize dictionaries to store results\n","all_task_dfs = {}\n","all_trialized_data = {}\n","\n","# Load data for each mouse and date\n","for mouse in mouse_list:\n","    #print(f\"\\nLoading data for mouse {mouse}\")\n","    mouse_task_dfs = []\n","    mouse_trialized_data = []\n","\n","    for date in date_lists[mouse]:\n","        try:\n","            #print(f\"\\nAttempting to load date: {date}\")\n","            task_df, trialized_data = behavior_analyzer.load_session_data(\n","                mouse_name=mouse,\n","                date=date,\n","                verbose=False\n","            )\n","\n","            # Add date identifier to task_df\n","            task_df['date'] = date\n","\n","            # Filter out trials that are within 5 trials after a context switch\n","            filtered_task_df = task_df.copy()\n","            context_changes = filtered_task_df['context'].diff().ne(0)\n","\n","            # Create a counter that resets at each context change\n","            trials_since_change = np.zeros(len(filtered_task_df))\n","            counter = 0\n","            for i in range(len(filtered_task_df)):\n","                if context_changes.iloc[i]:\n","                    counter = 0\n","                trials_since_change[i] = counter\n","                counter += 1\n","\n","            filtered_task_df['trials_since_change'] = trials_since_change\n","\n","            # Keep only trials that are at least 5 trials after a context switch\n","            filtered_task_df = filtered_task_df[filtered_task_df['trials_since_change'] >= 0]\n","\n","            # Store results for this date\n","            mouse_task_dfs.append(filtered_task_df)\n","            mouse_trialized_data.append(trialized_data)\n","\n","            # Print session info\n","            #print(f\"Successfully loaded {len(task_df)} trials\")\n","            #print(f\"Performance: {task_df['outcome'].mean():.1%}\")\n","\n","        except Exception as e:\n","            print(f\"Error loading data for date {date}: {str(e)}\")\n","            continue\n","\n","    if mouse_task_dfs:  # Only store if we successfully loaded any sessions\n","        # Combine all sessions for this mouse\n","        all_task_dfs[mouse] = pd.concat(mouse_task_dfs, ignore_index=True)\n","\n","        # Store trialized data as a list of sessions\n","        all_trialized_data[mouse] = mouse_trialized_data\n","\n","        # Print summary for this mouse\n","        print(f\"\\nSummary for mouse {mouse}:\")\n","        print(f\"Total sessions: {len(mouse_task_dfs)}\")\n","        print(f\"Total trials: {len(all_task_dfs[mouse])}\")\n","        print(f\"Overall performance: {all_task_dfs[mouse]['outcome'].mean():.1%}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-J1j5E5ad6M"},"outputs":[],"source":["all_task_dfs['IS-2-1L']"]},{"cell_type":"markdown","metadata":{"id":"j1vbbPh3ad6M"},"source":["# Task Accuracy Across Contexts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Msal_0jad6M"},"outputs":[],"source":["def plot_context_accuracy_with_sessions(task_df):\n","    \"\"\"\n","    Plot mean accuracy across contexts with individual session averages connected by lines.\n","\n","    Args:\n","        task_df (pd.DataFrame): DataFrame containing trial information for one mouse\n","\n","    Returns:\n","        tuple: (fig, ax) matplotlib figure and axes objects\n","    \"\"\"\n","    fig, ax = plt.subplots(figsize=(3.5, 2.5), dpi=800)\n","\n","    context_names = ['Congruent', 'Visual', 'Audio']\n","    context_colors = {0: 'purple', 1: '#EC008C', 2: '#27AAE1'}\n","\n","    # Calculate overall means and SEMs for each context\n","    means = []\n","    sems = []\n","    for ctx in [0, 1, 2]:\n","        ctx_trials = task_df[task_df['context'] == ctx]\n","        mean_acc = ctx_trials['outcome'].mean() * 100\n","        sem_acc = scipy.stats.sem(ctx_trials['outcome']) * 100\n","        means.append(mean_acc)\n","        sems.append(sem_acc)\n","\n","    # Plot overall means with error bars\n","    x_positions = np.arange(len(context_names))\n","    ax.bar(x_positions, means,\n","           color=[context_colors[i] for i in range(3)],\n","           alpha=0.7)\n","    #ax.errorbar(x_positions, means, yerr=sems,\n","    #           fmt='none', color='black', capsize=5)\n","\n","    # Plot individual session data and connect them\n","    for session_date in task_df['date'].unique():\n","        session_df = task_df[task_df['date'] == session_date]\n","        session_means = []\n","\n","        # Calculate mean for each context in this session\n","        for ctx in [0, 1, 2]:\n","            ctx_trials = session_df[session_df['context'] == ctx]\n","            if len(ctx_trials) > 0:\n","                session_means.append(ctx_trials['outcome'].mean() * 100)\n","            else:\n","                session_means.append(np.nan)\n","\n","        # Plot points and connecting lines\n","        ax.plot(x_positions, session_means,\n","                color='gray', alpha=0.3, linewidth=0.5)\n","\n","        # Plot individual points\n","        for ctx in [0, 1, 2]:\n","            if not np.isnan(session_means[ctx]):\n","                ax.scatter(x_positions[ctx], session_means[ctx],\n","                          color=context_colors[ctx],\n","                          s=30, alpha=0.7,\n","                          edgecolor='white', linewidth=0.5)\n","\n","    # Customize plot\n","    ax.set_xticks(x_positions)\n","    ax.set_xticklabels(context_names)\n","    ax.set_ylabel('Accuracy (%)')\n","    ax.set_ylim(40, 100)\n","\n","    # Add chance line\n","    ax.axhline(y=50, color='gray', linestyle=':', alpha=0.5)\n","\n","    # Remove top and right spines\n","    ax.spines['top'].set_visible(False)\n","    ax.spines['right'].set_visible(False)\n","\n","    plt.tight_layout()\n","    return fig, ax"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"No6nbqmzad6N"},"outputs":[],"source":["fig, ax = plot_context_accuracy_with_sessions(all_task_dfs['IS-2-1L'])\n","plt.savefig(os.path.join(figures_dir, 'imaging_animals_performance.svg'), format='svg', bbox_inches='tight')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Qfei-0E1ad6N"},"source":["# Dynamic Choice Model - Kinematic Data"]},{"cell_type":"markdown","metadata":{"id":"gP953VNrad6N"},"source":["## Initialize Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6H6qe2elad6N"},"outputs":[],"source":["# Prepare data for LSTM model\n","def prepare_data(task_df, trialized_data):\n","    # Get position and velocity data (5 channels)\n","    X = trialized_data['X_pos']  # Shape: (n_trials, timesteps)\n","    Y = trialized_data['Y_pos']\n","    View = trialized_data['View']\n","    dX = trialized_data['X_velocity']\n","    dY = trialized_data['Y_velocity']\n","\n","    # Stack all channels\n","    X_data = np.stack([X, Y, View, dX, dY], axis=2)  # Shape: (n_trials, timesteps, 5)\n","\n","    # Get choices (already binary: 0 for left, 1 for right)\n","    y_data = np.array(task_df['choice'])\n","\n","    # Remove trials with abnormal length (> 2x average)\n","    trial_lengths = np.array([len(trial) for trial in X])\n","    avg_length = np.mean(trial_lengths)\n","    mask = trial_lengths <= 2 * avg_length\n","\n","    return X_data[mask], y_data[mask]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cElrFOaDad6O"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Device selection for Apple Silicon (M1/M2) GPU\n","device = (\n","    torch.device(\"mps\")\n","    if torch.backends.mps.is_available()\n","    else torch.device(\"cpu\")\n",")\n","print(f\"Using device: {device}\")\n","\n","class MovementDataset(Dataset):\n","    \"\"\"Custom Dataset for movement data\"\"\"\n","    def __init__(self, X, y):\n","        self.X = torch.FloatTensor(X)\n","        self.y = torch.FloatTensor(y)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","def normalize_data(all_sessions_data):\n","    \"\"\"Normalize kinematic features across all sessions\"\"\"\n","    # Collect all kinematic data to compute global stats\n","    all_X = np.concatenate([session['X_data'] for session in all_sessions_data], axis=0)\n","\n","    # Compute mean and std for each feature\n","    mean = np.mean(all_X, axis=(0, 1))  # mean across trials and timesteps\n","    std = np.std(all_X, axis=(0, 1))    # std across trials and timesteps\n","\n","    # Normalize each session's data\n","    normalized_sessions = []\n","    for session in all_sessions_data:\n","        normalized_session = session.copy()\n","        normalized_session['X_data'] = (session['X_data'] - mean) / (std + 1e-8)\n","        normalized_sessions.append(normalized_session)\n","\n","    return normalized_sessions, mean, std\n","\n","class Dynamic_Choice_CNN(nn.Module):\n","    \"\"\"CNN model for predicting choices at each timestep\"\"\"\n","    def __init__(self, n_features):\n","        super(Dynamic_Choice_CNN, self).__init__()\n","\n","        # Convolutional layers with padding='same' to maintain temporal dimension\n","        self.conv_layers = nn.Sequential(\n","            # First conv block\n","            nn.Conv1d(n_features, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm1d(32),\n","            nn.ReLU(),\n","\n","            # Second conv block\n","            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm1d(64),\n","            nn.ReLU(),\n","\n","            # Third conv block\n","            nn.Conv1d(64, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm1d(32),\n","            nn.ReLU(),\n","        )\n","\n","        # Point-wise prediction layer\n","        self.prediction_layer = nn.Conv1d(32, 1, kernel_size=1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, timesteps, n_features)\n","        # Transpose for CNN: (batch_size, n_features, timesteps)\n","        x = x.transpose(1, 2)\n","\n","        # Apply conv layers while maintaining temporal dimension\n","        x = self.conv_layers(x)\n","\n","        # Final prediction at each timestep\n","        x = self.prediction_layer(x)\n","        x = self.sigmoid(x)\n","\n","        # Return to (batch_size, timesteps) format\n","        return x.squeeze(1)\n","\n","def train_model_leave_one_session_out_CNN(all_sessions_data, device=None):\n","    \"\"\"Training loop with normalized data\"\"\"\n","    if device is None:\n","        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","\n","    # Normalize data\n","    normalized_sessions, mean, std = normalize_data(all_sessions_data)\n","    predictions = []\n","\n","    for test_session_idx in range(len(normalized_sessions)):\n","        test_session = normalized_sessions[test_session_idx]\n","        test_X = test_session['X_data']\n","        test_y = test_session['y_data']\n","        test_date = test_session['date']\n","\n","        print(f\"\\n=== Training model {test_session_idx + 1}/{len(normalized_sessions)} ===\")\n","        print(f\"Test session date: {test_date}\")\n","\n","        # Combine other sessions for training\n","        train_X_list = []\n","        train_y_list = []\n","        for train_session_idx in range(len(normalized_sessions)):\n","            if train_session_idx != test_session_idx:\n","                train_X_list.append(normalized_sessions[train_session_idx]['X_data'])\n","                train_y_list.append(normalized_sessions[train_session_idx]['y_data'])\n","\n","        train_X = np.concatenate(train_X_list, axis=0)\n","        train_y = np.concatenate(train_y_list)\n","\n","        # Expand y to match timesteps\n","        train_y_expanded = np.repeat(train_y[:, np.newaxis], train_X.shape[1], axis=1)\n","        test_y_expanded = np.repeat(test_y[:, np.newaxis], test_X.shape[1], axis=1)\n","\n","        # Create datasets and dataloaders\n","        train_dataset = MovementDataset(train_X, train_y_expanded)\n","        test_dataset = MovementDataset(test_X, test_y_expanded)\n","\n","        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","        # Initialize model\n","        model = Dynamic_Choice_CNN(\n","            n_features=train_X.shape[2]\n","        ).to(device)\n","\n","        criterion = nn.BCELoss()\n","        optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","        # Training loop\n","        n_epochs = 50\n","        best_val_loss = float('inf')\n","        patience = 10\n","        patience_counter = 0\n","\n","        for epoch in range(n_epochs):\n","            # Training phase\n","            model.train()\n","            train_loss = 0.0\n","            for batch_X, batch_y in train_loader:\n","                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","\n","                optimizer.zero_grad()\n","                outputs = model(batch_X)\n","                loss = criterion(outputs, batch_y)\n","                loss.backward()\n","                optimizer.step()\n","\n","                train_loss += loss.item()\n","\n","            # Validation phase\n","            model.eval()\n","            val_loss = 0.0\n","            with torch.no_grad():\n","                for batch_X, batch_y in test_loader:\n","                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","                    outputs = model(batch_X)\n","                    val_loss += criterion(outputs, batch_y).item()\n","\n","            avg_train_loss = train_loss / len(train_loader)\n","            avg_val_loss = val_loss / len(test_loader)\n","\n","            if (epoch + 1) % 5 == 0:\n","                print(f'Epoch [{epoch+1}/{n_epochs}], '\n","                      f'Train Loss: {avg_train_loss:.4f}, '\n","                      f'Val Loss: {avg_val_loss:.4f}')\n","\n","            # Early stopping\n","            if avg_val_loss < best_val_loss:\n","                best_val_loss = avg_val_loss\n","                patience_counter = 0\n","            else:\n","                patience_counter += 1\n","\n","            if patience_counter >= patience:\n","                print(f\"Early stopping triggered at epoch {epoch+1}\")\n","                break\n","\n","        # Generate predictions\n","        model.eval()\n","        session_predictions = []\n","        with torch.no_grad():\n","            for batch_X, _ in test_loader:\n","                batch_X = batch_X.to(device)\n","                outputs = model(batch_X)\n","                session_predictions.append(outputs.cpu().numpy())\n","\n","        session_predictions = np.concatenate(session_predictions, axis=0)\n","        predictions.append((test_date, session_predictions))\n","\n","        # Clear memory\n","        model = model.cpu()\n","\n","    return predictions"]},{"cell_type":"markdown","metadata":{"id":"coDwE2GNad6O"},"source":["## Train and Make Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4lQUg7yad6O"},"outputs":[],"source":["all_sessions_data = []\n","\n","print(\"Processing sessions for each mouse:\")\n","for mouse in mouse_list:\n","    print(f\"\\nMouse: {mouse}\")\n","\n","    # Get all unique dates for this mouse\n","    mouse_dates = sorted(all_task_dfs[mouse]['date'].unique())\n","    print(f\"Dates in task_df: {mouse_dates}\")\n","    print(f\"Number of trialized sessions: {len(all_trialized_data[mouse])}\")\n","\n","    # Process each session\n","    for session_idx, trialized_data in enumerate(all_trialized_data[mouse]):\n","        # Get corresponding task_df for this session\n","        date = mouse_dates[session_idx]  # Use the date in order\n","        session_df = all_task_dfs[mouse][all_task_dfs[mouse]['date'] == date]\n","\n","        print(f\"\\nProcessing session {session_idx + 1} for date {date}\")\n","        print(f\"Number of trials in behavioral data: {len(session_df)}\")\n","        print(f\"Number of trials in trialized data: {len(trialized_data)}\")\n","\n","        # Prepare data\n","        try:\n","            X_data, y_data = prepare_data(session_df, trialized_data)\n","\n","            # Store session data\n","            session_info = {\n","                'X_data': X_data,\n","                'y_data': y_data,\n","                'date': date\n","            }\n","            all_sessions_data.append(session_info)\n","            print(f\"Successfully added session for date {date}\")\n","            print(f\"X_data shape: {X_data.shape}, y_data shape: {y_data.shape}\")\n","        except Exception as e:\n","            print(f\"Error preparing data for date {date}: {e}\")\n","\n","print(\"\\nFinal all_sessions_data summary:\")\n","print(f\"Total number of sessions: {len(all_sessions_data)}\")\n","for i, session in enumerate(all_sessions_data):\n","    print(f\"\\nSession {i}:\")\n","    print(f\"Date: {session['date']}\")\n","    print(f\"X_data shape: {session['X_data'].shape}\")\n","    print(f\"y_data shape: {session['y_data'].shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gWYzpzZad6O"},"outputs":[],"source":["all_predictions = train_model_leave_one_session_out_CNN(all_sessions_data, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lE9IjGFKad6O"},"outputs":[],"source":["def convert_predictions_to_array(predictions_list):\n","    \"\"\"\n","    Convert list of (session_date, predictions) tuples to a single numpy array.\n","\n","    Args:\n","        predictions_list: List of tuples (session_date, predictions) from model\n","\n","    Returns:\n","        numpy.ndarray: Concatenated predictions array\n","    \"\"\"\n","    all_predictions = []\n","    for _, session_pred in predictions_list:\n","        if isinstance(session_pred, np.ndarray):\n","            if session_pred.ndim == 3:\n","                session_pred = session_pred.squeeze()\n","            all_predictions.append(session_pred)\n","\n","    if all_predictions:\n","        return np.concatenate(all_predictions, axis=0)\n","    else:\n","        raise ValueError(\"No valid predictions found in the list\")\n","\n","def calculate_latency_to_choice(predictions_input, all_choices, threshold=0.95):\n","    \"\"\"\n","    Calculate latency to choice for each trial based on when model prediction\n","    crosses threshold.\n","\n","    Args:\n","        predictions_input: Either a numpy array of predictions or list of (date, predictions) tuples\n","        all_choices: Array of actual choices (0 or 1)\n","        threshold: Confidence threshold for choice (default 0.8)\n","\n","    Returns:\n","        numpy.ndarray: Array of latencies (as percentage of trial duration)\n","                      -1 indicates threshold never crossed\n","    \"\"\"\n","    # Handle input format\n","    if isinstance(predictions_input, list):\n","        # Convert from list of tuples format\n","        all_predictions = []\n","        for _, session_pred in predictions_input:\n","            if session_pred.ndim == 3:\n","                session_pred = session_pred.squeeze()\n","            all_predictions.append(session_pred)\n","        predictions = np.concatenate(all_predictions, axis=0)\n","    else:\n","        # Input is already a numpy array\n","        predictions = predictions_input\n","        if predictions.ndim == 3:\n","            predictions = predictions.squeeze()\n","\n","    n_trials = len(predictions)\n","    trial_length = predictions.shape[1]  # Number of timepoints per trial\n","    latencies = np.full(n_trials, -1.)  # Initialize all latencies to -1\n","\n","    # Calculate latency for each trial\n","    for i in range(n_trials):\n","        pred = predictions[i]\n","        if all_choices[i] == 1:  # Right choice\n","            # Find first timepoint where prediction > threshold\n","            thresh_crossed = np.where(pred > threshold)[0]\n","        else:  # Left choice\n","            # Find first timepoint where prediction < (1-threshold)\n","            thresh_crossed = np.where(pred < (1-threshold))[0]\n","\n","        if len(thresh_crossed) > 0:\n","            # Convert frame number to percentage of trial duration\n","            latencies[i] = (thresh_crossed[0] / trial_length) * 100\n","\n","    return latencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWGk3EEAad6P"},"outputs":[],"source":["# Get corresponding choices from the task dataframe\n","all_choices = []\n","for mouse in mouse_list:\n","    all_choices.extend(all_task_dfs[mouse]['choice'].values)\n","all_choices = np.array(all_choices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f97t2qWcad6P"},"outputs":[],"source":["# Now you can use this before any function that expects a numpy array:\n","predictions_array = convert_predictions_to_array(all_predictions)\n","\n","# Then use predictions_array in your function calls\n","if predictions_array.ndim == 3:\n","    predictions_array = predictions_array.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FrdN2oEqad6P"},"outputs":[],"source":["# Calculate latency to choice\n","latencies = calculate_latency_to_choice(predictions_array, all_choices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHsg3FSnad6P"},"outputs":[],"source":["def plot_mean_latency_by_context(task_df, latencies):\n","    \"\"\"\n","    Plot mean latency for each context with error bars and individual session data points.\n","\n","    Args:\n","        task_df (pd.DataFrame): DataFrame containing trial information including contexts\n","        latencies (np.array): Array of latency values for each trial\n","\n","    Returns:\n","        tuple: (fig, ax) matplotlib figure and axes objects\n","    \"\"\"\n","    fig, ax = plt.subplots(figsize=(3.5, 2.5), dpi=800)\n","\n","    context_names = ['Congruent', 'Visual', 'Audio']\n","    context_colors = {0: 'purple', 1: '#EC008C', 2: '#27AAE1'}\n","\n","    # Calculate overall mean latency for each context\n","    means = []\n","    sems = []\n","\n","    for ctx in [0, 1, 2]:\n","        # Get latencies for this context\n","        ctx_mask = task_df['context'] == ctx\n","        ctx_latencies = latencies[ctx_mask]\n","        valid_latencies = ctx_latencies[ctx_latencies >= 0]  # Exclude invalid (-1) latencies\n","\n","        mean_lat = np.mean(valid_latencies) if len(valid_latencies) > 0 else np.nan\n","        sem_lat = scipy.stats.sem(valid_latencies) if len(valid_latencies) > 0 else np.nan\n","\n","        means.append(mean_lat)\n","        sems.append(sem_lat)\n","\n","    # Plot bars\n","    x_positions = np.arange(len(context_names))\n","    ax.bar(x_positions, means,\n","           color=[context_colors[i] for i in range(3)],\n","           alpha=0.3)  # Reduced alpha to make individual points more visible\n","\n","    # Add error bars\n","    ax.errorbar(x_positions, means, yerr=sems,\n","               fmt='none', color='black', capsize=5)\n","\n","    # Plot individual session data points\n","    for ctx in [0, 1, 2]:\n","        # Calculate mean latency for each session\n","        for session_date in task_df['date'].unique():\n","            session_mask = (task_df['date'] == session_date) & (task_df['context'] == ctx)\n","            session_latencies = latencies[session_mask]\n","            valid_latencies = session_latencies[session_latencies >= 0]\n","\n","            if len(valid_latencies) > 0:\n","                session_mean = np.mean(valid_latencies)\n","                # Add jitter to x-position to avoid overlapping points\n","                jitter = np.random.normal(0, 0.05)\n","                ax.scatter(x_positions[ctx] + jitter, session_mean,\n","                          color=context_colors[ctx],\n","                          s=50,  # Point size\n","                          alpha=0.7,\n","                          edgecolor='white',\n","                          linewidth=0.5)\n","\n","    # Customize plot\n","    ax.set_xticks(x_positions)\n","    ax.set_xticklabels(context_names)\n","    ax.set_ylabel('Latency to choice\\n(% of maze)')\n","    ax.set_ylim(20, 85)\n","\n","    # Remove top and right spines\n","    ax.spines['top'].set_visible(False)\n","    ax.spines['right'].set_visible(False)\n","\n","    plt.tight_layout()\n","    return fig, ax"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JCWCf9ecad6P"},"outputs":[],"source":["fig, ax = plot_mean_latency_by_context(all_task_dfs['IS-2-1L'], latencies)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2eljbtYsad6P"},"outputs":[],"source":["def plot_example_trajectories_predictions(predictions, all_choices, all_task_dfs, mouse_list, n_examples=1):\n","    \"\"\"\n","    Plot example model prediction trajectories for each context, showing one left and one right choice.\n","\n","    Args:\n","        predictions (np.array): Model predictions over time (n_trials, n_timepoints)\n","        all_choices (np.array): Actual choices made (n_trials)\n","        all_task_dfs (dict): Dictionary of task DataFrames\n","        mouse_list (list): List of mouse IDs\n","        n_examples (int): Number of example trajectories to plot per choice per context\n","\n","    Returns:\n","        tuple: (fig, axs) matplotlib figure and axes objects\n","    \"\"\"\n","    fig, axs = plt.subplots(1, 3, figsize=(12, 4), dpi=800)\n","\n","    context_names = ['Congruent', 'Visual', 'Audio']\n","    context_colors = {0: 'purple', 1: '#EC008C', 2: '#27AAE1'}\n","\n","    # Get task_df\n","    task_df = pd.concat([all_task_dfs[mouse] for mouse in mouse_list], ignore_index=True)\n","\n","    # Create maze position array (0-100%)\n","    maze_positions = np.linspace(0, 100, predictions.shape[1])\n","\n","    for ctx_idx, ctx in enumerate([0, 1, 2]):\n","        ax = axs[ctx_idx]\n","\n","        # Get trials for this context\n","        ctx_mask = task_df['context'] == ctx\n","        ctx_trials = np.where(ctx_mask)[0]\n","\n","        # Get left and right choice trials\n","        left_trials = ctx_trials[all_choices[ctx_trials] == 0]\n","        right_trials = ctx_trials[all_choices[ctx_trials] == 1]\n","\n","        # Select random correct trials for each choice\n","        correct_left = left_trials[task_df.iloc[left_trials]['outcome'] == 1]\n","        correct_right = right_trials[task_df.iloc[right_trials]['outcome'] == 1]\n","\n","        # Randomly select trials\n","        np.random.seed(45)  # For reproducibility\n","        selected_left = np.random.choice(correct_left, size=min(n_examples, len(correct_left)), replace=False)\n","        selected_right = np.random.choice(correct_right, size=min(n_examples, len(correct_right)), replace=False)\n","\n","        # Plot trajectories\n","        for trial_idx in selected_left:\n","            ax.plot(maze_positions, predictions[trial_idx],\n","                   color=context_colors[ctx],\n","                   alpha=0.7,\n","                   linestyle='-',\n","                   label='Left choice' if trial_idx == selected_left[0] else '')\n","\n","        for trial_idx in selected_right:\n","            ax.plot(maze_positions, predictions[trial_idx],\n","                   color=context_colors[ctx],\n","                   alpha=0.7,\n","                   linestyle='--',\n","                   label='Right choice' if trial_idx == selected_right[0] else '')\n","\n","        # Add horizontal line at 0.5\n","        ax.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5)\n","\n","        # Customize plot\n","        ax.set_title(context_names[ctx_idx])\n","        ax.set_xlabel('% of maze')\n","        ax.set_ylabel('P(Right)' if ctx_idx == 0 else '')\n","\n","        # Set axis limits\n","        ax.set_ylim(-0.1, 1.1)\n","        ax.set_xlim(0, 100)\n","\n","        # Remove spines\n","        ax.spines['top'].set_visible(False)\n","        ax.spines['right'].set_visible(False)\n","\n","        # Add legend for first plot only\n","        if ctx_idx == 0:\n","            ax.legend(frameon=False)\n","\n","    plt.tight_layout()\n","    return fig, axs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AAUFXC6ad6P"},"outputs":[],"source":["fig, axs = plot_example_trajectories_predictions(predictions_array, all_choices, all_task_dfs, mouse_list, n_examples=3)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"eQrXekMAad6P"},"source":["# Dynamic Choice - Neural Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nzm0C1aHad6Q"},"outputs":[],"source":["class NeuralDynamicCNN(nn.Module):\n","    \"\"\"CNN model for predicting choices from neural data at each timestep\"\"\"\n","    def __init__(self, n_neurons, dropout_rate=0.2):\n","        super(NeuralDynamicCNN, self).__init__()\n","\n","        # First convolutional block with larger kernel\n","        self.conv1 = nn.Sequential(\n","            nn.Conv1d(n_neurons, 64, kernel_size=5, padding=2),\n","            nn.BatchNorm1d(64),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        # Residual blocks\n","        self.res_block1 = ResidualBlock(64, 128)\n","        self.res_block2 = ResidualBlock(128, 128)\n","\n","        # Attention mechanism\n","        self.attention = TemporalAttention(128)\n","\n","        # Final prediction layer\n","        self.prediction_layer = nn.Conv1d(128, 1, kernel_size=1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, frames, n_neurons)\n","        # Transpose for CNN: (batch_size, n_neurons, frames)\n","        x = x.transpose(1, 2)\n","\n","        # Apply convolutional blocks\n","        x = self.conv1(x)\n","        x = self.res_block1(x)\n","        x = self.res_block2(x)\n","\n","        # Apply attention\n","        x = self.attention(x)\n","\n","        # Final prediction at each timestep\n","        x = self.prediction_layer(x)\n","        x = self.sigmoid(x)\n","\n","        # Return to (batch_size, frames) format\n","        return x.squeeze(1)\n","\n","\n","class ResidualBlock(nn.Module):\n","    \"\"\"Residual block for the CNN\"\"\"\n","    def __init__(self, in_channels, out_channels):\n","        super(ResidualBlock, self).__init__()\n","\n","        # Main path\n","        self.conv_path = nn.Sequential(\n","            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm1d(out_channels),\n","            nn.ReLU(),\n","            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm1d(out_channels)\n","        )\n","\n","        # Skip connection\n","        self.skip = nn.Sequential()\n","        if in_channels != out_channels:\n","            self.skip = nn.Sequential(\n","                nn.Conv1d(in_channels, out_channels, kernel_size=1),\n","                nn.BatchNorm1d(out_channels)\n","            )\n","\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        # Apply main path\n","        out = self.conv_path(x)\n","        # Apply skip connection\n","        out = out + self.skip(x)\n","        # Apply ReLU\n","        out = self.relu(out)\n","        return out\n","\n","\n","class TemporalAttention(nn.Module):\n","    \"\"\"Temporal attention mechanism\"\"\"\n","    def __init__(self, channels):\n","        super(TemporalAttention, self).__init__()\n","\n","        self.query = nn.Conv1d(channels, channels // 8, kernel_size=1)\n","        self.key = nn.Conv1d(channels, channels // 8, kernel_size=1)\n","        self.value = nn.Conv1d(channels, channels, kernel_size=1)\n","\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, channels, frames)\n","        batch_size, channels, frames = x.size()\n","\n","        # Query, key, value projections\n","        proj_query = self.query(x).view(batch_size, -1, frames).permute(0, 2, 1)  # (B, F, C')\n","        proj_key = self.key(x).view(batch_size, -1, frames)  # (B, C', F)\n","        proj_value = self.value(x)  # (B, C, F)\n","\n","        # Attention map\n","        energy = torch.bmm(proj_query, proj_key)  # (B, F, F)\n","        attention = self.softmax(energy)  # (B, F, F)\n","\n","        # Apply attention\n","        out = torch.bmm(proj_value, attention.permute(0, 2, 1))  # (B, C, F)\n","\n","        # Residual connection\n","        out = self.gamma * out + x\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nFx1nCH9ad6Q"},"outputs":[],"source":["# Initialize model and criterion and optimizer\n","model = NeuralDynamicCNN(n_neurons=train_X.shape[2]).to(device)\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Set epochs and training mode\n","num_epochs = 5\n","model.train()\n","\n","# Train model\n","for epoch in range(num_epochs):\n","    optimizer.zero_grad()\n","    outputs = model(train_X)\n","    loss = criterion(outputs, train_y)\n","    loss.backward()\n","    optimizer.step()\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"]},{"cell_type":"code","source":["# Evaluate model\n","model.eval()\n","with torch.no_grad():\n","    test_outputs = model(test_X)\n","    test_loss = criterion(test_outputs, test_y)\n","    print(f'Test Loss: {test_loss.item():.4f}')"],"metadata":{"id":"LicApTer1AUy"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"runyan_lab_alignment","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}